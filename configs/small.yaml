model:
  vocab_size: 50257
  n_layer: 4
  n_head: 4
  n_embd: 128
  block_size: 256
  dropout: 0.1

training:
  batch_size: 32
  learning_rate: 3e-4
  max_epochs: 10
  warmup_steps: 100
  weight_decay: 0.01
  gradient_clip: 1.0

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  train_split: "train"
  val_split: "validation"
  max_length: 256

logging:
  log_interval: 100
  eval_interval: 500
  save_interval: 1000
  project_name: "minigpt"
  run_name: "small_model"