model:
  vocab_size: 50257
  n_layer: 6
  n_head: 6
  n_embd: 256
  block_size: 512
  dropout: 0.1

training:
  batch_size: 16
  learning_rate: 1e-4
  max_epochs: 15
  warmup_steps: 200
  weight_decay: 0.01
  gradient_clip: 1.0

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-103-raw-v1"
  train_split: "train"
  val_split: "validation"
  max_length: 512

logging:
  log_interval: 50
  eval_interval: 300
  save_interval: 500
  project_name: "minigpt"
  run_name: "medium_model"
  use_wandb: true