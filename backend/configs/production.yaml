model:
  vocab_size: 50257
  n_layer: 6
  n_head: 6
  n_embd: 256
  block_size: 512
  dropout: 0.1

training:
  batch_size: 16
  learning_rate: 1e-4
  max_epochs: 20
  warmup_steps: 500
  weight_decay: 0.01
  gradient_clip: 1.0

  # Better training dynamics
  lr_decay: true
  min_lr: 1e-6
  eval_every: 500

  # Improved sampling
  temperature: 0.8
  top_k: 40
  top_p: 0.9

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-103-raw-v1"  # Larger dataset
  train_split: "train"
  val_split: "validation"
  max_length: 512

  # Better tokenization
  add_special_tokens: true
  padding: true
  truncation: true

logging:
  log_interval: 100
  eval_interval: 500
  save_interval: 1000
  project_name: "minigpt"
  run_name: "production_model"

  # Enhanced logging
  save_best_only: true
  patience: 3
  monitor: "val_loss"